# How to Use the Mini-RAG System

## Quick Start

### Option 1: Interactive Query Interface (Easiest)

Run the interactive script to ask questions:

```bash
python query_rag.py
```

Then:
1. Choose whether to enable reranking (y/n)
2. Wait for documents to load and index to build
3. Ask your questions!
4. Type 'quit' or 'exit' to stop

**Example:**
```
Question: How many days of PTO do employees get?
Number of chunks to retrieve (default=3): 3
```

### Option 2: Run Example Code

See working examples:

```bash
python example_usage.py
```

This shows 3 different ways to use the system.

### Option 3: Use in Your Own Code

```python
from mini_rag import MiniRAG

# 1. Initialize
rag = MiniRAG(use_reranking=False)

# 2. Load your documents
rag.load_documents(['handbook.txt', 'faq.txt', 'blog.txt'])

# 3. Build the index (one-time setup)
rag.generate_embeddings()

# 4. Ask questions!
answer, chunks = rag.answer("What is the PTO policy?", k=3)
print(answer)
```

## Common Use Cases

### Ask a Single Question

```python
rag = MiniRAG()
rag.load_documents(['handbook.txt', 'faq.txt', 'blog.txt'])
rag.generate_embeddings()

answer, _ = rag.answer("How do I request time off?")
print(answer)
```

### Get Just the Relevant Chunks (No Answer Generation)

```python
rag = MiniRAG()
rag.load_documents(['handbook.txt', 'faq.txt', 'blog.txt'])
rag.generate_embeddings()

chunks = rag.retrieve("What are the working hours?", k=5)
for chunk in chunks:
    print(f"{chunk['metadata']['filename']}: {chunk['text']}")
```

### Use Reranking for Better Results

```python
rag = MiniRAG(use_reranking=True)  # Enable reranking
rag.load_documents(['handbook.txt', 'faq.txt', 'blog.txt'])
rag.generate_embeddings()

answer, chunks = rag.answer("What mental health resources are available?", k=3)
```

### Adjust Number of Retrieved Chunks

```python
# Get more chunks for comprehensive answers
answer, chunks = rag.answer("Tell me about company policies", k=5)

# Get fewer chunks for focused answers
answer, chunks = rag.answer("What is PTO?", k=1)
```

## Configuration Options

### Change Chunk Size

```python
rag = MiniRAG()
rag.chunker.token_limit = 500  # Default is 400
rag.chunker.overlap = 100      # Default is 50
```

### Use Different Embedding Model

```python
# Larger, better model (slower)
rag = MiniRAG(embedding_model="all-mpnet-base-v2")

# Default (faster)
rag = MiniRAG(embedding_model="all-MiniLM-L6-v2")
```

### Enable LLM Answers (Requires OpenAI API Key)

```bash
# Set your API key
export OPENAI_API_KEY="your-key-here"

# Or in Python:
import os
os.environ['OPENAI_API_KEY'] = 'your-key-here'
```

Then answers will be generated by GPT-3.5 instead of just concatenated chunks.

## File Structure

- `mini_rag.py` - Main RAG implementation
- `query_rag.py` - Interactive query interface
- `example_usage.py` - Code examples
- `test_rag.py` - Test suite
- `handbook.txt`, `faq.txt`, `blog.txt` - Sample documents

## Tips

1. **First run is slow**: Loading the embedding model takes time, but subsequent queries are fast
2. **Index is rebuilt each time**: For production, save/load the FAISS index
3. **Chunk size matters**: 300-500 tokens works well for most queries
4. **More chunks = more context**: But also more noise, so balance k=3-5
5. **Reranking helps**: Especially for ambiguous queries, but adds computation time
